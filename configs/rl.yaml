# RL (Reinforcement Learning) Configuration

# PPO Algorithm Settings
ppo:
  # Training parameters
  episodes: 1000          # Number of training episodes
  max_steps: 50           # Maximum steps per episode
  batch_size: 64          # Batch size
  
  # PPO hyperparameters
  learning_rate: 3e-4     # Learning rate
  gamma: 0.99             # Discount factor
  clip_epsilon: 0.2       # PPO clipping parameter
  value_loss_coef: 0.5    # Value loss coefficient
  entropy_coef: 0.01      # Entropy regularization coefficient
  
  # Optimization settings
  update_epochs: 4        # Number of epochs per update
  gae_lambda: 0.95        # GAE lambda

# Actor-Critic Network Architecture
policy_network:
  # Network dimensions
  state_dim: 901          # 768 (BERT) + 1 (stance) + 1 (conviction) + 1 (round) + 128 (social) + 2 (history)
  hidden_size: 256
  
  # Strategy settings
  num_strategies: 4
  strategies:
    - name: "aggressive"
      id: 0
      description: "Aggressive attack - directly challenge opponent's arguments"
    - name: "defensive"
      id: 1
      description: "Defensive rebuttal - strengthen own arguments"
    - name: "analytical"
      id: 2
      description: "Analytical reasoning - rationally analyze all viewpoints"
    - name: "empathetic"
      id: 3
      description: "Empathetic persuasion - understand opponent's position"
  
  # Dropout
  dropout: 0.1

# Debate Environment Settings
environment:
  # Reward design
  reward_scale: 1.0           # Reward scaling
  persuasion_bonus: 5         # Successful persuasion reward
  surrender_penalty: -3       # Surrender penalty
  diversity_bonus: 0.1        # Strategy diversity bonus
  
  # Termination conditions
  max_rounds: 10              # Maximum number of rounds
  conviction_threshold: 0.3   # Conviction threshold (surrender condition)
  stance_threshold: 0.2       # Stance threshold (neutral condition)
  
  # State transitions
  stance_momentum: 0.8        # Stance change momentum
  conviction_decay: 0.05      # Conviction decay rate

# Training Settings
training:
  # Basic parameters
  save_frequency: 100         # Save every N episodes
  log_frequency: 10           # Log every N episodes
  evaluation_episodes: 50     # Number of episodes for evaluation
  
  # GPU settings
  device: "auto"  # auto, cuda, cpu
  
  # Optimizer settings
  optimizer:
    type: "adam"
    eps: 1e-5
  
  # Gradient clipping
  gradient_clipping: 0.5
  
  # Random seed
  seed: 42

# Experience Replay Settings
experience_replay:
  # Buffer size
  buffer_size: 10000
  
  # Sampling settings
  sample_size: 64
  
  # Prioritized experience replay
  prioritized:
    enabled: false
    alpha: 0.6
    beta: 0.4

# Evaluation Settings
evaluation:
  # Evaluation metrics
  metrics:
    - "average_reward"      # Average reward
    - "win_rate"           # Win rate
    - "persuasion_rate"    # Persuasion success rate
    - "strategy_diversity" # Strategy diversity
    - "episode_length"     # Average episode length
  
  # Evaluation frequency
  eval_frequency: 100      # Evaluate every N episodes
  
  # Report generation
  generate_report: true
  report_format: ["json", "png"]

# Inference Settings
inference:
  # Exploration strategy
  exploration:
    enabled: true
    method: "epsilon_greedy"  # epsilon_greedy, boltzmann
    epsilon: 0.1
    temperature: 1.0
    decay_rate: 0.995
    min_epsilon: 0.01
  
  # Strategy selection
  strategy_selection:
    # Consideration factors
    consider_history: true    # Consider historical strategies
    consider_opponent: true   # Consider opponent characteristics
    consider_context: true    # Consider dialogue context

# Model Saving Settings
checkpointing:
  save_dir: "data/models/ppo"
  save_best_only: false      # PPO usually saves multiple checkpoints
  save_frequency: 100        # Every N episodes
  keep_last_n: 5            # Keep last N checkpoints
  
  # Save items
  save_items:
    - "actor_model"
    - "critic_model"
    - "optimizer_state"
    - "training_stats"

# Logging Settings
logging:
  level: "INFO"
  log_dir: "logs/ppo"
  tensorboard: true
  wandb: false              # Optional: use Weights & Biases
